"""
ml_predictor.py - ML —Ñ–∏–ª—å—Ç—Ä –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Random Forest –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—ã
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ñ—å—é—á–µ—Ä—Å–æ–≤ Si/RTS
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pickle
import warnings
warnings.filterwarnings('ignore')


class MLPredictor:
    """
    ML –º–æ–¥–µ–ª—å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤

    –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç: UP (—Ü–µ–Ω–∞ –≤—ã—Ä–∞—Å—Ç–µ—Ç) –∏–ª–∏ DOWN (—Ü–µ–Ω–∞ —É–ø–∞–¥—ë—Ç)
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –∫ Breakout —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    """

    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_names = []
        self.accuracy = 0.0

    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –°–æ–∑–¥–∞—ë—Ç 30+ —Ñ–∏—á–∏ –¥–ª—è ML –º–æ–¥–µ–ª–∏

        –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:
        1. –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ (SMA, EMA)
        2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (RSI, MACD, Bollinger Bands)
        3. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (ATR)
        4. –û–±—ä—ë–º
        5. Momentum
        6. –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π
        """
        features = pd.DataFrame(index=df.index)

        # ==========================================
        # 1. –°–ö–û–õ–¨–ó–Ø–©–ò–ï –°–†–ï–î–ù–ò–ï
        # ==========================================
        for period in [5, 10, 20, 50, 100]:
            features[f'sma_{period}'] = df['close'].rolling(period).mean()
            features[f'price_to_sma_{period}'] = df['close'] / features[f'sma_{period}']

            # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —Å–∫–æ–ª—å–∑—è—â–∞—è
            features[f'ema_{period}'] = df['close'].ewm(span=period).mean()

        # ==========================================
        # 2. RSI (Relative Strength Index)
        # ==========================================
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        features['rsi_14'] = 100 - (100 / (1 + rs))

        # RSI —Ä–∞–∑–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤
        for period in [7, 21]:
            gain_p = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss_p = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs_p = gain_p / loss_p
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs_p))

        # ==========================================
        # 3. MACD (Moving Average Convergence Divergence)
        # ==========================================
        ema_12 = df['close'].ewm(span=12).mean()
        ema_26 = df['close'].ewm(span=26).mean()
        features['macd'] = ema_12 - ema_26
        features['macd_signal'] = features['macd'].ewm(span=9).mean()
        features['macd_histogram'] = features['macd'] - features['macd_signal']

        # ==========================================
        # 4. BOLLINGER BANDS
        # ==========================================
        sma_20 = df['close'].rolling(20).mean()
        std_20 = df['close'].rolling(20).std()
        features['bb_upper'] = sma_20 + (std_20 * 2)
        features['bb_lower'] = sma_20 - (std_20 * 2)
        features['bb_middle'] = sma_20
        features['bb_width'] = (features['bb_upper'] - features['bb_lower']) / features['bb_middle']
        features['bb_position'] = (df['close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])

        # ==========================================
        # 5. ATR (Average True Range) - –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        # ==========================================
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)
        features['atr_14'] = true_range.rolling(14).mean()
        features['atr_normalized'] = features['atr_14'] / df['close']

        # ==========================================
        # 6. –û–ë–™–Å–ú
        # ==========================================
        features['volume_sma_20'] = df['volume'].rolling(20).mean()
        features['volume_ratio'] = df['volume'] / features['volume_sma_20']
        features['volume_change'] = df['volume'].pct_change()

        # ==========================================
        # 7. MOMENTUM (–ò–º–ø—É–ª—å—Å)
        # ==========================================
        for period in [3, 5, 10, 20]:
            features[f'momentum_{period}'] = df['close'].pct_change(period)
            features[f'momentum_high_{period}'] = df['high'].pct_change(period)
            features[f'momentum_low_{period}'] = df['low'].pct_change(period)

        # ==========================================
        # 8. –°–í–ï–ß–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´
        # ==========================================
        # –†–∞–∑–º–µ—Ä —Ç–µ–ª–∞ —Å–≤–µ—á–∏
        features['candle_body'] = np.abs(df['close'] - df['open']) / df['open']

        # –í–µ—Ä—Ö–Ω—è—è/–Ω–∏–∂–Ω—è—è —Ç–µ–Ω—å
        features['upper_shadow'] = (df['high'] - df[['close', 'open']].max(axis=1)) / df['open']
        features['lower_shadow'] = (df[['close', 'open']].min(axis=1) - df['low']) / df['open']

        # –ë—ã—á—å—è/–º–µ–¥–≤–µ–∂—å—è —Å–≤–µ—á–∞
        features['is_bullish'] = (df['close'] > df['open']).astype(int)

        # ==========================================
        # 9. –õ–ê–ì–ò (–ü—Ä–æ—à–ª—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        # ==========================================
        for lag in [1, 2, 3, 5, 10]:
            features[f'return_lag_{lag}'] = df['close'].pct_change().shift(lag)
            features[f'volume_lag_{lag}'] = df['volume'].pct_change().shift(lag)

        # ==========================================
        # 10. –í–†–ï–ú–Ø –î–ù–Ø (–¥–ª—è –≤–Ω—É—Ç—Ä–∏–¥–Ω–µ–≤–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏)
        # ==========================================
        if hasattr(df.index, 'hour'):
            features['hour'] = df.index.hour
            features['minute'] = df.index.minute
            # –í—Ä–µ–º—è –∫–∞–∫ —Å–∏–Ω—É—Å–æ–∏–¥–∞ (—Ü–∏–∫–ª–∏—á–µ—Å–∫–∞—è —Ñ–∏—á–∞)
            features['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)
            features['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)

        # ==========================================
        # –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê
        # ==========================================
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
        features = features.fillna(method='bfill').fillna(method='ffill').fillna(0)

        # –ó–∞–º–µ–Ω—è–µ–º inf –Ω–∞ –±–æ–ª—å—à–∏–µ —á–∏—Å–ª–∞
        features.replace([np.inf, -np.inf], [999, -999], inplace=True)

        self.feature_names = features.columns.tolist()
        return features

    def prepare_training_data(self, df: pd.DataFrame, forward_periods: int = 5):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Args:
            df: –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            forward_periods: –ß–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ —Å–≤–µ—á–µ–π –ø—Ä–æ–≤–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç

        Returns:
            X (features), y (labels: 1=UP, 0=DOWN)
        """
        print(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")

        # –°–æ–∑–¥–∞—ë–º —Ñ–∏—á–∏
        X = self.create_features(df)

        # –°–æ–∑–¥–∞—ë–º —Ç–∞—Ä–≥–µ—Ç: —Ü–µ–Ω–∞ –≤—ã—Ä–∞—Å—Ç–µ—Ç –∏–ª–∏ —É–ø–∞–¥—ë—Ç —á–µ—Ä–µ–∑ N —Å–≤–µ—á–µ–π?
        future_returns = df['close'].shift(-forward_periods) / df['close'] - 1

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: UP (1) –µ—Å–ª–∏ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å > 0, –∏–Ω–∞—á–µ DOWN (0)
        y = (future_returns > 0).astype(int)

        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å—Ç—Ä–æ–∫ (–Ω–µ—Ç –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
        X = X.iloc[:-forward_periods]
        y = y.iloc[:-forward_periods]

        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ —Ç–∞—Ä–≥–µ—Ç–µ
        valid_idx = ~y.isna()
        X = X[valid_idx]
        y = y[valid_idx]

        print(f"   –í—Å–µ–≥–æ —Å—ç–º–ø–ª–æ–≤: {len(X)}")
        print(f"   –§–∏—á–µ–π: {X.shape[1]}")
        print(f"   UP: {y.sum()} ({y.sum()/len(y)*100:.1f}%)")
        print(f"   DOWN: {len(y)-y.sum()} ({(len(y)-y.sum())/len(y)*100:.1f}%)")

        return X, y

    def train(self, df: pd.DataFrame, forward_periods: int = 5, test_size: float = 0.2):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π

        Args:
            df: DataFrame —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            forward_periods: –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ (—Å–≤–µ—á–µ–π –≤–ø–µ—Ä—ë–¥)
            test_size: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (0.2 = 20%)
        """
        print("\n" + "="*70)
        print("ü§ñ –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ò")
        print("="*70)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self.prepare_training_data(df, forward_periods)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, shuffle=False  # –ù–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã!
        )

        print(f"\nüìà –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} —Å—ç–º–ø–ª–æ–≤")
        print(f"üìâ –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} —Å—ç–º–ø–ª–æ–≤")

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # –û–±—É—á–µ–Ω–∏–µ Random Forest
        print("\nüå≥ –û–±—É—á–µ–Ω–∏–µ Random Forest...")
        self.model = RandomForestClassifier(
            n_estimators=200,        # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤ = –ª—É—á—à–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ
            max_depth=15,            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã –ø—Ä–æ—Ç–∏–≤ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
            min_samples_split=50,    # –ú–∏–Ω–∏–º—É–º —Å—ç–º–ø–ª–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è
            min_samples_leaf=20,     # –ú–∏–Ω–∏–º—É–º —Å—ç–º–ø–ª–æ–≤ –≤ –ª–∏—Å—Ç–µ
            max_features='sqrt',     # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Ñ–∏—á–µ–π
            random_state=42,
            n_jobs=-1,               # –í—Å–µ —è–¥—Ä–∞ CPU
            class_weight='balanced'  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
        )

        self.model.fit(X_train_scaled, y_train)
        if train_accuracy - test_accuracy > 0.15:  # –†–∞–∑–Ω–∏—Ü–∞ >15%
            print("‚ö†Ô∏è –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï! –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –ø–æ–¥–æ–≥–Ω–∞–Ω–∞ –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ")
            print(f"   Train: {train_accuracy*100:.1f}% vs Test: {test_accuracy*100:.1f}%")
            print("   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —É–º–µ–Ω—å—à–∏—Ç–µ max_depth –∏–ª–∏ —É–≤–µ–ª–∏—á—å—Ç–µ min_samples_split")
        self.is_trained = True

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ
        y_train_pred = self.model.predict(X_train_scaled)
        train_accuracy = accuracy_score(y_train, y_train_pred)

        # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ (–í–ê–ñ–ù–û!)
        y_test_pred = self.model.predict(X_test_scaled)
        test_accuracy = accuracy_score(y_test, y_test_pred)
        self.accuracy = test_accuracy

        print("\n" + "="*70)
        print("‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø")
        print("="*70)
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {train_accuracy*100:.1f}%")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –¢–ï–°–¢–û–í–´–• –¥–∞–Ω–Ω—ã—Ö:  {test_accuracy*100:.1f}% ‚≠ê")
        print()

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        print("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (—Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞):")
        print(classification_report(y_test, y_test_pred, 
                                   target_names=['DOWN', 'UP'],
                                   digits=3))

        # Feature importance
        print("\nüîç –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
        importances = self.model.feature_importances_
        top_features = sorted(zip(self.feature_names, importances), 
                            key=lambda x: x[1], reverse=True)[:10]
        for i, (feat, imp) in enumerate(top_features, 1):
            print(f"   {i}. {feat:30s}: {imp:.4f}")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        print("\n" + "="*70)
        if test_accuracy >= 0.55:
            print("‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
            print("   –¢–æ—á–Ω–æ—Å—Ç—å >55% - —ç—Ç–æ —Ö–æ—Ä–æ—à–æ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ä—ã–Ω–∫–æ–≤")
        elif test_accuracy >= 0.52:
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é")
            print("   –¢–æ—á–Ω–æ—Å—Ç—å 52-55% - —Å–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª")
        else:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–≥–∞–¥—ã–≤–∞–Ω–∏—è")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
            print("   1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö (30+ –¥–Ω–µ–π)")
            print("   2. –ò–∑–º–µ–Ω–∏—Ç—å forward_periods")
            print("   3. –î–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏")
        print("="*70)

    def predict(self, df: pd.DataFrame) -> dict:
        """
        –î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö

        Returns:
            {
                'direction': 'UP' –∏–ª–∏ 'DOWN',
                'probability': –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (0-1),
                'confidence': 'HIGH', 'MEDIUM', 'LOW'
            }
        """
        if not self.is_trained:
            return {
                'direction': 'NEUTRAL', 
                'probability': 0.5, 
                'confidence': 'NONE'
            }

        # –°–æ–∑–¥–∞—ë–º —Ñ–∏—á–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–≤–µ—á–∏
        X = self.create_features(df)
        X_last = X.iloc[[-1]]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        X_scaled = self.scaler.transform(X_last)

        # –ü—Ä–æ–≥–Ω–æ–∑
        prediction = self.model.predict(X_scaled)[0]
        probabilities = self.model.predict_proba(X_scaled)[0]

        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
        prob = probabilities[prediction]

        # –£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if prob >= 0.70:
            confidence = 'HIGH'
        elif prob >= 0.60:
            confidence = 'MEDIUM'
        else:
            confidence = 'LOW'

        return {
            'direction': 'UP' if prediction == 1 else 'DOWN',
            'probability': float(prob),
            'confidence': confidence,
            'prob_up': float(probabilities[1]),
            'prob_down': float(probabilities[0])
        }

    def save_model(self, filepath: str = 'ml_model.pkl'):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'scaler': self.scaler,
                'feature_names': self.feature_names,
                'accuracy': self.accuracy
            }, f)
        print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")

    def load_model(self, filepath: str = 'ml_model.pkl'):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.model = data['model']
            self.scaler = data['scaler']
            self.feature_names = data['feature_names']
            self.accuracy = data.get('accuracy', 0.0)
            self.is_trained = True
        print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {self.accuracy*100:.1f}%")
